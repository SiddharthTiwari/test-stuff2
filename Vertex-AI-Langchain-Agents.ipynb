{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3582e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "/root/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"adventml\"  # @param {type:\"string\"}\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabb2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple, Union\n",
    "import re\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    BaseSingleActionAgent,\n",
    "    LLMSingleActionAgent,\n",
    "    AgentOutputParser,\n",
    "    Tool\n",
    ")\n",
    "from langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.llms.vertexai import VertexAI\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.tools.base import tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe522417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c15da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVertexAIEmbedding(VertexAIEmbeddings, Embeddings):\n",
    "    model_name: str = \"textembedding-gecko\"\n",
    "    max_batch_sizes: int = 5\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of strings.\n",
    "\n",
    "        Args:\n",
    "            texts: List[str] The list of strings to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        chunked_texts = [\n",
    "            texts[i : i + self.max_batch_sizes]\n",
    "            for i in range(0, len(texts), self.max_batch_sizes)\n",
    "        ]\n",
    "        embeddings = []\n",
    "\n",
    "        for chunk in chunked_texts:\n",
    "            embeddings.extend(self.client.get_embeddings(chunk))\n",
    "\n",
    "        return [el.values for el in embeddings]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a text.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embedding for the text.\n",
    "        \"\"\"\n",
    "        embeddings = self.client.get_embeddings([text])\n",
    "        return embeddings[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2051a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = MyVertexAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18f19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockTool(BaseTool):\n",
    "    name = \"MockTool\"\n",
    "    description = \"useful for when you need to answer questions about a person\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        print(f\"*** Invoking MockTool with query '{query}'\")\n",
    "        return f\"Answer of '{query}' is 'Michael Chi'\"\n",
    "\n",
    "    async def _arun(self, query: str) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        print(f\"*** Invoking MockTool with query '{query}'\")\n",
    "        return f\"Answer of '{query}' is 'Michael Chi'\"\n",
    "\n",
    "\n",
    "# return_direct=True | False\n",
    "@tool(\"MockTool_Function\", return_direct=False)\n",
    "def MockTool_Function(query: str) -> str:\n",
    "    \"\"\"\n",
    "    useful for when you need to answer questions about everything about Google PaLM2\n",
    "    \"\"\"\n",
    "    print(f\">MockTool_Functioon says {query}\")\n",
    "    return \"\"\"\n",
    "        PaLM 2 is our next generation large language model that builds on Google’s legacy\n",
    "        of breakthrough research in machine learning and responsible AI.\n",
    "        It excels at advanced reasoning tasks, including code and math,\n",
    "        classification and question answering, translation and multilingual proficiency,\n",
    "        and natural language generation better than our previous state-of-the-art LLMs,\n",
    "        including PaLM. It can accomplish these tasks because of the way it was built\n",
    "        – bringing together compute-optimal scaling, an improved dataset mixture,\n",
    "        and model architecture improvements.\n",
    "        PaLM 2 is grounded in Google’s approach to building and deploying AI responsibly.\n",
    "        It was evaluated rigorously for its potential harms and biases,\n",
    "        capabilities and downstream uses in research and in-product applications.\n",
    "\n",
    "        It’s being used in other state-of-the-art models, like Med-PaLM 2 and Sec-PaLM,\n",
    "        and is powering generative AI features and tools at Google,\n",
    "        like Bard and the PaLM API.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "@tool(\"MockTool_Function_Return_Direct\", return_direct=True)\n",
    "def MockTool_Function_Return_Direct(query: str) -> str:\n",
    "    \"\"\"\n",
    "    useful for when you need to answer questions about everything about Google Bard\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "        Bard is an experiment based on this same technology that lets you collaborate with generative AI.\n",
    "        As a creative and helpful collaborator, Bard can supercharge your imagination, boost your productivity,\n",
    "        and help you bring your ideas to life.\n",
    "\n",
    "        If you’re interested in the more technical details, LaMDA is a Transformer-based model,\n",
    "        the machine-learning breakthrough invented by Google in 2017Opens in a new window.\n",
    "\n",
    "        The language model learns by “reading” trillions of words that help it pick up on patterns that make up human language,\n",
    "        so it’s good at predicting what might be reasonable responses.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006ea08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbadb3ac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "class VertexPromptTemplate(StringPromptTemplate):\n",
    "    template: str = \"\"\"\n",
    "        You are an helpful agent.\n",
    "        Answer the following questions as best you can.\n",
    "        You have access to the following tools:\n",
    "\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "\n",
    "        Question: the input question you must answer\n",
    "        Thought: you should alJeeways think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: the final answer to the original input question.\n",
    "\n",
    "        Begin!\n",
    "\n",
    "        Question: {input}\n",
    "        {agent_scratchpad}\"\"\"\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        # This is the first iteration, there is no agent thoughts yet.\n",
    "        if \"agent_scratchpad\" not in kwargs:\n",
    "            kwargs[\"agent_scratchpad\"] = \"\"\n",
    "        # Retrieve action and observation of this iteration\n",
    "        for action, observation in intermediate_steps:\n",
    "            kwargs[\"agent_scratchpad\"] += action.log\n",
    "            kwargs[\"agent_scratchpad\"] += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"tools\"] = \"\\n\".join(\n",
    "            [f\"* {tool.name}: {tool.description}\" for tool in self.tools]\n",
    "        )\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "\n",
    "class VertexLLMOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        print(\"***** VertexLLMOutputParser::parse()::llm_output->{}\".format(llm_output))\n",
    "        # If we have the final answer\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # The output should be in the following format\n",
    "        # Action: \"action name\"\n",
    "        # Action Input: \"inputs\"\n",
    "        # Sometimes the LLM may produce Action Input without double quotes\n",
    "        # We need to handle this.\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input:$\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if match:\n",
    "            llm_output = llm_output + \"\\\"\\\"\"\n",
    "\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(\n",
    "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352aed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='MockTool', description='useful for when you need to answer questions about a person', func=<bound method BaseTool.run of MockTool(verbose=True)>), StructuredTool(name='MockTool_Function', description='MockTool_Function(query: str) -> str - useful for when you need to answer questions about everything about Google PaLM2', args_schema=<class 'pydantic.v1.main.MockTool_FunctionSchemaSchema'>, func=<function MockTool_Function at 0x7f86ffc6e940>)]\n"
     ]
    }
   ],
   "source": [
    "mock = MockTool(verbose=True)\n",
    "mock_func = MockTool_Function\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=mock.name,\n",
    "        func=mock.run,\n",
    "        description=mock.description,\n",
    "        # return_direct=True  # I want to return the result directly to the user\n",
    "    ),\n",
    "    mock_func,\n",
    "]\n",
    "\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff0a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "***** VertexLLMOutputParser::parse()::llm_output->Thought: I need to know who is the speaker of this session\n",
      "Action: MockTool\n",
      "Action Input: \n",
      "\u001b[32;1m\u001b[1;3mThought: I need to know who is the speaker of this session\n",
      "Action: MockTool\n",
      "Action Input: \u001b[0m*** Invoking MockTool with query ''\n",
      "\u001b[32;1m\u001b[1;3mAnswer of '' is 'Michael Chi'\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mAnswer of '' is 'Michael Chi'\u001b[0m\n",
      "***** VertexLLMOutputParser::parse()::llm_output->I now know the final answer\n",
      "Final Answer: Michael Chi\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Michael Chi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "Michael Chi\n"
     ]
    }
   ],
   "source": [
    "prompt = VertexPromptTemplate(\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "output_parser = VertexLLMOutputParser()\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")\n",
    "\n",
    "result = agent_executor.run(input=\"Who is the speaker of this session ?\")\n",
    "print(\"*****\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50ad29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0mSearch is not a valid tool, try one of [MockTool, MockTool_Function].\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent stopped due to iteration limit or time limit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Agent Class\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MyCustomAgent(BaseSingleActionAgent):\n",
    "    \"\"\"My Custom Agent.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "\n",
    "    def plan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        return AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\")\n",
    "\n",
    "    async def aplan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        return AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Run\n",
    "\"\"\"\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=MyCustomAgent(), tools=tools, verbose=True\n",
    ")\n",
    "result = agent_executor.run(\"How many people live in taiwan as of 2023?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eef93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
